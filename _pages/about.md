---
permalink: /
title: "ğŸ¦˜Teng-Fang â€“ Master Student in NYCU"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## ğŸ‘‹ Hi! Iâ€™m Hsiao, Teng-Fang (è•­ç™»æ–¹)

Iâ€™ve been pursuing my **Masterâ€™s degree of Electrical Engineering at National Yang Ming Chiao Tung University** since **September 2024** ğŸ“. Under the supervision of professor Shuai, Hong-Han.

My research focuses on **Computer Vision** and **AIGC**.  
I previously worked on **2D generation and image editing**, and I'm now exploring **3D and video generation** ğŸ§ ğŸ–¼ï¸ğŸ¥.

I'm the **first author** of publications at **ICCV 2025**, **AAAI 2025**, and **WACV 2024**,  
and also **co-author** of papers accepted at **ICIP 2025 (Oral)**, **EMNLP 2025 (Findings)** ğŸ“„âœ¨.

I'm always **open to academic discussions, collaborations, or research opportunities ğŸ¤** â€” feel free to reach out!

<hr style="width: 100%; border: none; border-top: 1px solid #ccc; margin: 30px 0;">

<div style="display: flex; flex-wrap: wrap; align-items: flex-start; gap: 10px;">
  <img src="images/tf_ti2i.jpg" alt="teaser" style="width: 100%; max-width: 800px; height: auto;">
  <div style="flex: 1; min-width: 200px;">
    <b style="display: inline-block; max-width: 100%; word-break: break-word;">
    TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models
    </b><br>
    <i><u>Teng-Fang Hsiao</u>, Bo-Kai Ruan, Yi-Lun Wu, Tzu-Ling Lin, Hong-Han Shuai</i> <br>
    <i>To be apper on International Conference on Computer Vision (ICCV), 2025</i> <br>
    Augment your T2I models with arbitrary number of images as references in a Training-Free manner ğŸ¦–ğŸ–¼ï¸ğŸ”¥<br> 
    <a href="https://arxiv.org/abs/2503.15283">pdf</a> /
    <a href="https://github.com/BlueDyee/TF-TI2I">GitHub</a> /
    <a href="https://bluedyee.github.io/TF-TI2I_page/">Project page</a>
  </div>
</div>

<hr style="width: 100%; border: none; border-top: 1px solid #ccc; margin: 30px 0;">

<div style="display: flex; flex-wrap: wrap; align-items: flex-start; gap: 10px;">
  <img src="images/tf_gph.jpg" alt="teaser" style="width: 100%; max-width: 800px; height: auto;">
  <div style="flex: 1; min-width: 200px;">
    <b style="display: inline-block; max-width: 100%; word-break: break-word;">
    Training-and-Prompt-Free General Painterly Harmonization via Zero-Shot Disentenglement on Style and Content References
    </b><br>
    <i><u>Teng-Fang Hsiao</u>, Bo-Kai Ruan, Hong-Han Shuai</i><br>
    <i>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2025</i><br>
    Harmonizing any copy and pasted objects in a Training-Free manner ğŸ–¼ï¸ğŸ‘©ğŸ»â€ğŸ¨ğŸ¨<br> 
    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32368">pdf</a> /
    <a href="https://github.com/BlueDyee/TF-GPH">GitHub</a>
  </div>
</div>

<hr style="width: 100%; border: none; border-top: 1px solid #ccc; margin: 30px 0;">

<div style="display: flex; flex-wrap: wrap; align-items: flex-start; gap: 10px;">
  <img src="images/light_attack.jpg" alt="teaser" style="width: 100%; max-width: 800px; height: auto;">
  <div style="flex: 1; min-width: 200px;">
    <b style="display: inline-block; max-width: 100%; word-break: break-word;">
    Natural light can also be dangerous: Traffic sign misinterpretation under adversarial natural light attacks
    </b><br>
    <i><u>Teng-Fang Hsiao</u>, Bo-Lun Huang, Zi-Xiang Ni, Yan-Ting Lin, Hong-Han Shuai, Yung-Hui Li, Wen-Huang Cheng</i> <br>
    <i>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024</i> <br>
    Real world physical attack on Autonomus Driving with Natural Light ğŸ’¡ğŸš—âš ï¸<br> 
    <a href="https://openaccess.thecvf.com/content/WACV2024/html/Hsiao_Natural_Light_Can_Also_Be_Dangerous_Traffic_Sign_Misinterpretation_Under_WACV_2024_paper.html">pdf</a> /
    <a href="https://github.com/BlueDyee/natural-light-attack">GitHub</a>
  </div>
</div>

<hr style="width: 100%; border: none; border-top: 1px solid #ccc; margin: 30px 0;">

<div style="display: flex; flex-wrap: wrap; align-items: flex-start; gap: 10px;">
  <img src="images/freecond.jpg" alt="teaser" style="width: 100%; max-width: 800px; height: auto;">
  <div style="flex: 1; min-width: 200px;">
    <b style="display: inline-block; max-width: 100%; word-break: break-word;">
    FreeCond: Free Lunch in the Input Conditions of Text-Guided Inpainting
    </b><br>
    <i><u>Teng-Fang Hsiao</u>, Bo-Kai Ruan, Sung-Lin Tsai, Yi-Lun Wu, Hong-Han Shuai </i><br>
    <i>arXiv preprint, 2024/11 </i><br>
    Enhancing Inpainting Quality without additional computation ğŸ–Œï¸ğŸ’ª0ï¸âƒ£<br> 
    <a href="https://arxiv.org/abs/2412.00427">pdf</a> /
    <a href="https://github.com/BlueDyee/natural-light-attack">GitHub</a>
  </div>
</div>

<hr style="width: 100%; border: none; border-top: 1px solid #ccc; margin: 30px 0;">
<hr style="width: 100%; border: none; border-top: 1px solid #ccc; margin: 30px 0;">

<div style="display: flex; flex-wrap: wrap; align-items: flex-start; gap: 10px;">
  <img src="images/rap.jpg" alt="teaser" style="width: 100%; max-width: 800px; height: auto;">
  <div style="flex: 1; min-width: 200px;">
    <b style="display: inline-block; max-width: 100%; word-break: break-word;">
    Not All Thats Rare Is Lost: Causal Paths to Rare Concept Synthesis
    </b><br>
    <i>Bo-Kai Ruan, Zi-Xiang Ni, Bo-Lun Huang, <u>Teng-Fang Hsiao</u>, Hong-Han Shuai </i><br>
    <i>arXiv preprint, 2025/5 </i><br>
    Rare Prompt T2I Generation ğŸ‘½ğŸ‘¾ğŸ¤–<br> 
    <a href="https://arxiv.org/abs/2505.20808">pdf</a> /
  </div>
</div>

<hr style="width: 100%; border: none; border-top: 1px solid #ccc; margin: 30px 0;">

<div style="display: flex; flex-wrap: wrap; align-items: flex-start; gap: 10px;">
  <img src="images/break_reviewer.jpg" alt="teaser" style="width: 100%; max-width: 800px; height: auto;">
  <div style="flex: 1; min-width: 200px;">
    <b style="display: inline-block; max-width: 100%; word-break: break-word;">
    Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks
    </b><br>
    <i>Tzu-Ling Lin, Wei-Chih Chen, <u>Teng-Fang Hsiao</u>, Hou-I Liu, Ya-Hsin Yeh, Yu Kai Chan, Wen-Sheng Lien, Po-Yen Kuo, Philip S Yu, Hong-Han Shuai</i><br>
    <i>To be appear on EMNLP Findings, 2025 </i><br>
    LLM Reviewers Are Not Robust against adversarial attacks ğŸ“œğŸ¤œğŸ¤–<br> 
    <a href="https://arxiv.org/abs/2506.11113">pdf</a> /
  </div>
</div>

<hr style="width: 100%; border: none; border-top: 1px solid #ccc; margin: 30px 0;">

<div style="display: flex; flex-wrap: wrap; align-items: flex-start; gap: 10px;">
  <img src="images/iterdiff.jpg" alt="teaser" style="width: 100%; max-width: 800px; height: auto;">
  <div style="flex: 1; min-width: 200px;">
    <b style="display: inline-block; max-width: 100%; word-break: break-word;">
    IterDiff: Training-Free Iterative Face Editing Via Efficient Clip-Guided Memory Bank
    </b><br>
    <i>Chun-Yao Chiu, Feng-Kai Huang, <u>Teng-Fang Hsiao</u>, Hong-Han Shuai, Wen-Huang Cheng</i><br>
    <i>IEEE International Conference on Image Processing (ICIP), 2025 </i><br>
    Overcome Image Corruption during Iteratively Image Editing made by IP2P ğŸ§ ğŸ¦ğŸ¨<br> 
    <a href="https://ieeexplore.ieee.org/abstract/document/11084536">pdf</a> /
  </div>
</div>
